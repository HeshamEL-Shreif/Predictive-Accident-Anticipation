{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<contextlib.ExitStack at 0x16b0889d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.backends.cudnn as cudnn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision.models import VGG16_Weights, efficientnet_v2_m, EfficientNet_V2_M_Weights\n",
    "import time\n",
    "from collections import deque\n",
    "from IPython.display import clear_output\n",
    "from feature_extraction import *\n",
    "from DSTA import *\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_probability_bar(frame, probability, position=(50, 50), bar_width=100, bar_height=10):\n",
    "    bar_length = int(bar_width * probability)\n",
    "    end_point = (position[0] + bar_length, position[1] + bar_height)\n",
    "    cv2.rectangle(frame, position, end_point, (0, 255, 0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_detector = YOLO('yolov9e.pt')  # load an official model\n",
    "object_detector = YOLO('best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classNames = ['Bus', 'Car', 'Truck', 'Motorcycle', 'Pedestrain', 'Truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = init_feature_extractor(backbone='efficientnet', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSTA(\n",
       "  (dsa): SpatialAttention()\n",
       "  (dta): TemporalAttention()\n",
       "  (gru): GatedRecurrentUnit(\n",
       "    (gru): GRU(1024, 512, batch_first=True)\n",
       "    (dense1): Linear(in_features=512, out_features=64, bias=True)\n",
       "    (dense2): Linear(in_features=64, out_features=2, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (tsaa): SelfAttentionAggregation(\n",
       "    (dense1): Linear(in_features=512, out_features=64, bias=True)\n",
       "    (dense2): Linear(in_features=64, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsta = DSTA(device=device, d=512, m=5, input_dim=512+512, output_dim=2, n_layers=1, t=50, n_objects=12).to(torch.float32)\n",
    "dsta.load_state_dict(torch.load('best_dsta.pth', map_location=device)['dsta_state_dict'])\n",
    "dsta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((640, 640)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-06 00:17:38.209 python[85208:10227796] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "/Users/heshamelsherif/miniconda3/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DSTA.__init__() missing 6 required positional arguments: 'm', 'input_dim', 'n_layers', 't', 'output_dim', and 'n_objects'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m frame_tensor \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(frame_tensor)\n\u001b[1;32m     40\u001b[0m frame_tensor \u001b[38;5;241m=\u001b[39m transform(frame_tensor)\n\u001b[0;32m---> 43\u001b[0m out, h, t_pred \u001b[38;5;241m=\u001b[39m \u001b[43mDSTA\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_tsaa\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m draw_probability_bar(frame, out[:,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     47\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWebcam\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n",
      "\u001b[0;31mTypeError\u001b[0m: DSTA.__init__() missing 6 required positional arguments: 'm', 'input_dim', 'n_layers', 't', 'output_dim', and 'n_objects'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        cap.set(3, 640)\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            '''\n",
    "            results = object_detector(frame, stream=True)\n",
    "\n",
    "            # coordinates\n",
    "            for r in results:\n",
    "                boxes = r.boxes\n",
    "\n",
    "                for box in boxes:\n",
    "                    # bounding box\n",
    "                    x1, y1, x2, y2 = box.xyxy[0]\n",
    "                    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2) # convert to int values\n",
    "\n",
    "                    # put box in cam\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "\n",
    "                    # confidence\n",
    "                    confidence = math.ceil((box.conf[0]*100))/100\n",
    "                    print(\"Confidence --->\",confidence)\n",
    "\n",
    "                    # class name\n",
    "                    cls = int(box.cls[0])\n",
    "                    print(\"Class name -->\", classNames[cls])\n",
    "\n",
    "                    # object details\n",
    "                    org = [x1, y1]\n",
    "                    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                    fontScale = 1\n",
    "                    color = (255, 0, 0)\n",
    "                    thickness = 2\n",
    "\n",
    "                    cv2.putText(frame, classNames[cls], org, font, fontScale, color, thickness)\n",
    "                    '''\n",
    "            frame_tensor = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_tensor = Image.fromarray(frame_tensor)\n",
    "            frame_tensor = transform(frame_tensor)\n",
    "\n",
    "\n",
    "            out, h, t_pred = DSTA(frame_tensor.unsqueeze(0), torch.tensor([[0, 1]]), with_tsaa=False)\n",
    "            draw_probability_bar(frame, out[:,1])\n",
    "            \n",
    "\n",
    "            cv2.imshow('Webcam', frame)\n",
    "            if cv2.waitKey(1) == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
